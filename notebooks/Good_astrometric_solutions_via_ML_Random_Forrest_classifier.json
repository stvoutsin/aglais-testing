{"paragraphs":[{"text":"%md\n\n# Using ML to define an astrometrically clean sample of stars\n\n   Follows Gaia EDR3 performance verification paper DPACP-81 (Smart et al.) in classifying astrometric solutions as good or bad\n   via supervised ML. Employs a Random Forrest classifier plus appropriately defined training sets - see\n\n   https://gaia.esac.esa.int/dpacsvn/DPAC/docs/DpacPublications/DR3PerformanceVerification/DPACP-81/main_submitted2308.pdf\n\n   (DPAC password protected) for further details. The work flow implemented here follows closely that described in Section 2, \"GCNS Generation\"\n   (GCNS = Gaia Catalogue of Nearby Stars) and is designed to clean up a 100pc (= nearby) sample.\n\n   Presently implemented for Gaia DR2; deploy and check this implementation against GEDR3 when released - it should reproduce what's in the paper.\n\n   <i>Version employing newer, richer dataframe API in pyspark ML</i>\n   \n   <b>IMPORTANT NOTE: </b> current deployment has Spark 2.4.7 installed. That specific version's API is documented here:\n   \n   https://spark.apache.org/docs/2.4.7/ml-classification-regression.html#random-forest-classifier\n   \n   Beware of following on-line message board and other fora posts for help and examples as they more often than not describe and link to different versions, and the API is evolving <i>all the time</i>.\n   \n   ","user":"gaiauser","dateUpdated":"2020-11-26T10:06:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963366_2005713725","id":"20201013-131059_546082898","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-26T10:06:07+0000","dateFinished":"2020-11-26T10:06:07+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2066"},{"text":"%spark.pyspark\n\n# this is the set of astrometric features to be used. In reality several iterations of this workflow might be required with an expanded set, and some figure-of-merit,\n# e.g. Gini index, would be used to select those most important to the RF classification - cf. Table A.1 in the GCNS paper.\nastrometric_features = [\n    'parallax_error', \n    'parallax_over_error',\n    'astrometric_sigma5d_max',\n    'pmra_error',\n    'pmdec_error',\n    'astrometric_excess_noise',\n    #'ipd_gof_harmonic_amplitude',\n    #'ruwe',                             TODO: reinstate all at GEDR3 !!!\n    'visibility_periods_used',\n    'pmdec',\n    'pmra',\n    #'ipd_frac_odd_win',\n    #'ipd_frac_multi_peak',\n    'astrometric_gof_al',\n    #'scan_direction_strength_k2',\n    'parallax_pmdec_corr'\n]\n# ... the last two are included to cross check against the Gini index results presented in the paper.\n\n# quick mode: set an additional predicate filter on random_index here to limit to 1% or 0.1% sampling etc:\nquick_filter = ''# AND MOD(random_index, 10) = 0'\n# ... to switch this off, simply specify an empty string. But to avoid overloading matplotlib when visualising results, keep this one:\nquick_plot_filter = ' AND MOD(random_index, 10) = 0'\n\n# Default Spark worker configuration cannot atm handle the full dataset - maybe this is (at least part of) the problem:\n# https://stackoverflow.com/questions/25707784/why-does-a-job-fail-with-no-space-left-on-device-but-df-says-otherwise\n\n# reformat the above attribute list into an SQL comma-separated select string\nfeatures_select_string = ('%s, '*(len(astrometric_features) - 1) + '%s ')%tuple(astrometric_features)\n#print (features_select_string)\n\n# Confirmed by Luis Sarro, personal communication: actually we train on ABS(parallax_over_error), see e.g. GCNS paper Figure A.5\nfeatures_select_string = features_select_string.replace('parallax_over_error','ABS(parallax_over_error) AS parallax_over_error')\n\n# photometric consistency predicate - e.g. Evans et al. (2018), Babusiaux et al. (2018) for DR2:\nphotometric_consistency_filter = ' AND phot_bp_rp_excess_factor BETWEEN 1.0 + (0.03 * POW(bp_rp, 2.0)) AND 1.3 + (0.06 * POW(bp_rp, 2.0))'\n\n# define the data source\ngs_df = sqlContext.read.parquet('/data/gaia/*.parquet')\n\n# register as SQL-queryable \ngs_df.createOrReplaceTempView('gaia_source')\n\n","user":"gaiauser","dateUpdated":"2020-11-26T10:02:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963368_-1981729652","id":"20201013-131649_1734629667","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T16:56:55+0000","dateFinished":"2020-11-25T16:57:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2067"},{"text":"%spark.pyspark\n\n# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)\nsqlContext.clearCache()\n\n# a conservative selection of everything that COULD be within 100pc, including things with measured \n# distances putting them outside the 100pc horizon when their true distances are within, and also including \n# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:\nraw_sources_df = spark.sql('SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, ' + features_select_string + 'FROM gaia_source WHERE ABS(parallax) > 8.0')\n\n# cache it for speedy access below (all subsequent samples are derived from this):\nraw_sources_df.cache()\n\n# register as SQL-queryable\nraw_sources_df.createOrReplaceTempView('raw_sources')\n\nraw_sources_df.count()\n# (cf. GEDR3: 1,211,740 sources) ","user":"gaiauser","dateUpdated":"2020-11-26T10:02:47+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963370_2129974015","id":"20201013-132418_278702125","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T16:57:33+0000","dateFinished":"2020-11-25T16:59:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2068"},{"text":"%spark.pyspark\n\n# plot an observational Hertzsprung-Russell diagram (aka colour / absolute magnitude diagram) for the unclassified sample to show the problem,\n# include the photometric consistency filter to show the problem is astrometric in addition to photometric\nunclassified_camd_df = spark.sql('SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp FROM raw_sources WHERE parallax > +8.0' + quick_plot_filter)# + photometric_consistency_filter)\n\nimport matplotlib.pyplot as plot\nplot.figure(0, figsize = (6.0, 9.7))\nx = list(unclassified_camd_df.select('g_rp').toPandas()['g_rp'])\ny = list(unclassified_camd_df.select('m_g').toPandas()['m_g'])\nplot.scatter(x, y, marker = '.', s = 1)\nplot.ylim(21.0, -3.0)\nplot.ylabel('Stellar brightness (absolute G magnitude) -->', fontsize = 16)\nplot.xlabel('<-- Stellar temperature (G - RP magnitude)', fontsize = 16)\nplot.show()\n\n","user":"gaiauser","dateUpdated":"2020-11-25T16:59:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963370_-937914806","id":"20201120-094650_221463065","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T16:59:31+0000","dateFinished":"2020-11-25T17:05:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2069"},{"text":"%md\n\nThe problem: while we see astrophysically interesting locii in this diagram, the lower right (cool, low temperature) regime <br> is dominated by systematic errors (not random uncertainties - the data should be equally precise in all parts of this data <br> space) that contaminate the raw sample. We wish to clean the sample to obtain high reliability\n\n* without compromising completeness;\n* utilising astrometric quality features in the raw catalogue for a volume-complete sample;\n* and efficiently; \n\ni.e. without endless iterations of manual, subjective, axis-parallel and arbitrary cuts on available catalogue attributes. A neat solution to this is to use supervised ML. In the Gaia EDR3 performance verification paper \"Gaia Catalogue of Nearby Stars\" (Smart, Sarro, Rybicki, et al. 2020) we use a Random Forest of decision trees on selected features having first defined a training set based on the data itself. \n\nNote that plotting the <i>intrinsic brightness</i> of a star as above requires determination of the <i>distance</i> to it along with a measurement of it's apparent brightness. Stellar distance determination is a fundamental goal of the Gaia mission and is achieved via measurement of the <i>stellar parallax</i>, the apparent \"wobble\" in angular position exhibited by all stars as seen from our (annually changing) view point in the solar system. The sample plotted above is selected for parallax > 8 milliarcseconds (mas) which corresponds to a distance within 125 parsecs (since a star at distance 1 parsec exhibits a parallax of 1 arcsecond; parsec = \"parallax arcsecond\"; 1 parsec is around 3.3 light-years).\n\nAn 8 mas training set of \"good\" examples is \"cleaned\" of highly probable spurious sources using <i>independent</i> photometric criteria (i.e. we require consistency of optical and infrared colours). The \"bad\" examples are selected having (unphysical) parallax < -8 mas, i.e. using parallax measurements that are formally highly significant, yet obviously spurious. Under the assumption of normally distributed uncertainties on the parallax measurements, this bad sample should be representative of the corresponding spurious measurements having parallax > 8 mas that contaminate the parallax-selected sample and, in particular, create the contamination illustrated in the plot above.\n","user":"gaiauser","dateUpdated":"2020-11-24T08:52:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963372_1994972809","id":"20201120-110502_1704727157","dateCreated":"2020-11-24T08:52:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2070"},{"text":"%spark.pyspark\n\n# good training data: first define rough positional cuts to exclude crowded regions at low Gaialctic latitude, and inside the Large and Small Magellanic Clouds (Luri et al. 2020):\nlow_galactic_latitude_filter = ' AND ABS(b) > 25.0'\nsmc_filter = ' AND (dec < -80.0 OR dec > -65.0 OR (ra < 350.0 AND ra > +40.0))'\nlmc_filter = ' AND (dec < -80.0 OR dec > -55.0 OR ra < 40.0 OR ra > 120.0)'\nall_good_training_df = spark.sql('SELECT 1 AS label, ' + features_select_string + ' FROM raw_sources WHERE parallax > + 8.0 AND ABS(b) > 25.0' + photometric_consistency_filter + quick_filter + low_galactic_latitude_filter + smc_filter + lmc_filter)\ngood_training_rows = all_good_training_df.count()\n#print('Good training data size: %d rows'%(good_training_rows))\n\n# bad training data: negative parallaxes: N.B. make a selection exactly the same size as the good training set based on size of smaller (good) data set and count of all available bads\nmaximal_bad_ast_count = spark.sql('SELECT source_id FROM raw_sources WHERE parallax < -8.0').count()\nfilter_factor = int(maximal_bad_ast_count / good_training_rows)\nall_bad_training_df = spark.sql('SELECT 0 AS label, ' + features_select_string + ' FROM raw_sources WHERE  parallax < -8.0 AND MOD(random_index, %d) = 0'%(filter_factor) + ' ORDER BY random_index LIMIT %d'%(good_training_rows))\n#all_bad_training_data_count = all_bad_training_df.count()\n#print ('Bad  training data size: %d rows'%(all_bad_training_data_count))\n","user":"gaiauser","dateUpdated":"2020-12-07T16:59:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963373_98475139","id":"20201123-105445_95907042","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T17:05:35+0000","dateFinished":"2020-11-25T17:10:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2071"},{"text":"%spark.pyspark\n\n# define training (67%) and test (33%) sample splits (seeded randomness for repeatability)\ngood_67pc, good_33pc = all_good_training_df.randomSplit([0.67, 0.33], 42)\nbad_67pc, bad_33pc = all_bad_training_df.randomSplit([0.67, 0.33], 42)\n\n# transform to labelled feature vectors (0.0 = bad, 1.0 = good, as conveniently already defined in previous projections above)\n\n# Annotate and transform appropriate to the input required by the classifier's API.\n# Need a dataframe with labels and features: use vector assembler. \nfrom pyspark.ml.feature import VectorAssembler\nignore = ['label',]\nassembler = VectorAssembler(inputCols=[x for x in good_67pc.columns if x not in ignore], outputCol='features')\n\n# training sets\ngood_training_df = assembler.transform(good_67pc).drop(*astrometric_features)\nbad_training_df = assembler.transform(bad_67pc).drop(*astrometric_features)\n# ... N.B. the original individual feature columns are dropped to save memory (since they are duplicated into the resulting feature vector).\n\n# testing sets\ngood_testing_df = assembler.transform(good_33pc).drop(*astrometric_features)\nbad_testing_df = assembler.transform(bad_33pc).drop(*astrometric_features)\n\n# concatenate the training set into a single dataframe\ntraining_df = good_training_df.union(bad_training_df)\n#training_df.show()\n","user":"gaiauser","dateUpdated":"2020-12-07T16:59:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963375_1659740506","id":"20201015-161110_18118893","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T17:11:43+0000","dateFinished":"2020-11-25T17:11:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2072"},{"text":"%spark.pyspark\n\n# This cell does the business, given the data and training sets. Follows the example Python code at \n# https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier\n\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# instantiate a trained RF classifier, seeded for repeatability at this stage:\nrf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 5000, impurity = 'gini', seed=42)\nmodel = rf.fit(training_df)\n\n# benchmarks: featureSubsetStrategy = \"sqrt\"\n# 10% sample,  100 trees:  3min 58sec (Tues) 7min 57sec (Wed) ...!\n","user":"gaiauser","dateUpdated":"2020-11-25T17:12:12+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963376_-1848568621","id":"20201013-152110_1282917873","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T17:12:12+0000","dateFinished":"2020-11-25T18:01:29+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2073"},{"text":"%spark.pyspark\n\n# classify based on the above trained model\ngood_test_results = model.transform(good_testing_df)\nbad_test_results = model.transform(bad_testing_df)\n\n#good_test_results.show()\n\n\n","user":"gaiauser","dateUpdated":"2020-11-25T16:37:52+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963376_-1116057798","id":"20201015-131823_1744793710","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T14:47:11+0000","dateFinished":"2020-11-25T14:47:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2074"},{"text":"%spark.pyspark\n\n# test results numerical output\n\n# count up\nfrom collections import Counter\npositives = Counter(list(good_test_results.select('prediction').toPandas()['prediction']))\nnegatives = Counter(list(bad_test_results.select('prediction').toPandas()['prediction']))\n\n# Confusion matrix (after GCNS paper, Table 1):\ntrue_positives = positives[1.0]\nfalse_positives = positives[0.0]\ntrue_negatives = negatives[0.0]\nfalse_negatives = negatives[1.0]\nprint('   |%7d%7d'%(1,2))\nprint('------------------------------')\nprint(' 1 |%7d%7d'%(true_positives, false_positives))\nprint(' 2 |%7d%7d'%(false_negatives, true_negatives))\nprint()\n\n# Misclassification fraction: cf. GCNS paper which quotes 0.1%\nnum_misclassified = false_positives + false_negatives\ntotal_num_in_test = true_positives + true_negatives + num_misclassified\nmisclassified_pc = 100.0 * float(num_misclassified) / float(total_num_in_test)\nprint('Misclassifications for the test set: %.2f %%'%(misclassified_pc))\n","user":"gaiauser","dateUpdated":"2020-11-25T14:48:44+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963376_1479126773","id":"20201016-154755_24366630","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T14:47:31+0000","dateFinished":"2020-11-25T14:47:44+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2075"},{"text":"%spark.pyspark\n\n# examine relative importance of features wrt Appendix A.1 of the GCNS paper\nfeature_relative_importance = model.featureImportances.toArray()\nprint('Relative importance of astrometric features:\\n')\nfor idx in range(len(astrometric_features)): print('%25s  :  %f'%(astrometric_features[idx], feature_relative_importance[idx]))\n","user":"gaiauser","dateUpdated":"2020-11-25T14:49:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963377_-954351192","id":"20201123-163421_1811049882","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T14:49:03+0000","dateFinished":"2020-11-25T14:49:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2076"},{"text":"%spark.pyspark\n\n# cleaned up CAMD (observational HRD) employing the classifications\n\n# get the complete unclassified sample:\nunclassified_sample_df = spark.sql('SELECT * FROM raw_sources WHERE parallax > +8.0' + quick_filter)\n\n# required features subset for the classification model\nassembler = VectorAssembler(inputCols=[x for x in unclassified_sample_df.columns if x in astrometric_features], outputCol='features')\ndf_to_classify = assembler.transform(unclassified_sample_df)\nall_classifications = model.transform(df_to_classify)\n#all_classifications.show()\n\n# register as SQL-queryable:\nall_classifications.createOrReplaceTempView('classified_sources')\n\n# select on binary classification for a quick check:\ngood_sources_df = spark.sql('SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp, ra, dec FROM classified_sources WHERE prediction=1.0' + quick_plot_filter)# + photometric_consistency_filter)\nbad_sources_df =  spark.sql('SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp, ra, dec FROM classified_sources WHERE prediction=0.0' + quick_plot_filter)# + photometric_consistency_filter)\n\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize = (6.0, 9.7))\nx = list(bad_sources_df.select('g_rp').toPandas()['g_rp'])\ny = list(bad_sources_df.select('m_g').toPandas()['m_g'])\nplot.scatter(x, y, marker = '.', s = 1, c = 'orange')\nx = list(good_sources_df.select('g_rp').toPandas()['g_rp'])\ny = list(good_sources_df.select('m_g').toPandas()['m_g'])\nplot.scatter(x, y, marker = '.', s = 1)\nplot.ylim(21.0, -3.0)\nplot.ylabel('Stellar brightness (absolute G magnitude) -->', fontsize = 16)\nplot.xlabel('<-- Stellar temperature (G - RP magnitude)', fontsize = 16)\nplot.show()\n\n\n","user":"gaiauser","dateUpdated":"2020-11-25T16:36:30+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606207963377_-2090513373","id":"20201123-162249_1468741293","dateCreated":"2020-11-24T08:52:43+0000","dateStarted":"2020-11-25T14:49:40+0000","dateFinished":"2020-11-25T14:50:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2077"},{"text":"%spark.pyspark\n\n# histogram of the classification probabilities: cf. GCNS paper Figure 3\n\n#all_classifications.show()\n\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize = (9.7, 6.0))\nplot.yscale('log')\nx = list(all_classifications.select('probability').toPandas()['probability'])\nplot.hist(x, bins=25, color='black')\nplot.xlabel('Random Forest Probability')\nplot.ylabel('N')\n\n","user":"gaiauser","dateUpdated":"2020-12-02T10:14:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606212312380_2125797250","id":"20201124-100512_110153564","dateCreated":"2020-11-24T10:05:12+0000","dateStarted":"2020-11-25T14:50:41+0000","dateFinished":"2020-11-25T14:50:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2078"},{"text":"%spark.pyspark\n\n# cf. GCNS paper Figure 1 panels: sky distribution of good/bad sources:\n\nimport math\n\nplot.figure(2, figsize = (16.18, 10.0))\nplot.subplot(111, projection='aitoff')\nplot.grid(True)\nx = list((good_sources_df.select('ra').toPandas()['ra'] - 180.0) * math.pi / 180.0)\ny = list(good_sources_df.select('dec').toPandas()['dec'] * math.pi / 180.0)\nplot.title('Good sources')\nplot.scatter(x, y, marker = '.', s = 1)\n","user":"gaiauser","dateUpdated":"2020-11-25T16:34:25+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606300246694_657398034","id":"20201125-103046_1353183691","dateCreated":"2020-11-25T10:30:46+0000","dateStarted":"2020-11-25T16:34:03+0000","dateFinished":"2020-11-25T16:34:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2079"},{"text":"%spark.pyspark\n\nplot.figure(3, figsize = (16.18, 10.0))\nplot.subplot(111, projection='aitoff')\nplot.grid(True)\nx = list((bad_sources_df.select('ra').toPandas()['ra'] - 180.0) * math.pi / 180.0)\ny = list(bad_sources_df.select('dec').toPandas()['dec'] * math.pi / 180.0)\nplot.title('Bad sources')\nplot.scatter(x, y, marker = '.', s = 1)\n","user":"gaiauser","dateUpdated":"2020-11-25T16:36:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606321992863_2117699663","id":"20201125-163312_728555601","dateCreated":"2020-11-25T16:33:12+0000","dateStarted":"2020-11-25T16:34:45+0000","dateFinished":"2020-11-25T16:35:02+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2080"},{"text":"%spark.pyspark\nprint(\"No. of good sources: \",good_sources_df.count())\nprint(\"No. of bad sources:  \",bad_sources_df.count())\n","user":"gaiauser","dateUpdated":"2020-11-25T16:44:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606319491215_1853015224","id":"20201125-155131_269531128","dateCreated":"2020-11-25T15:51:31+0000","dateStarted":"2020-11-25T15:53:44+0000","dateFinished":"2020-11-25T15:54:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2081"},{"text":"%spark.pyspark\n\n# histogram of distribution of parallax_over_error\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize = (9.7, 6.0))\nplot.yscale(\"log\")\nx = list(good_67pc.select(\"parallax_over_error\").toPandas()[\"parallax_over_error\"])\ny = list(bad_67pc.select(\"parallax_over_error\").toPandas()[\"parallax_over_error\"])\nplot.hist(x, bins=25, label='good')\nplot.hist(y, bins=25, label='bad')\nplot.xlabel(\"parallax_over_error\")\nplot.ylabel(\"Frequency\")\nplot.legend(loc='upper right')","user":"gaiauser","dateUpdated":"2020-11-25T16:45:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606234305041_148538781","id":"20201124-161145_1933006801","dateCreated":"2020-11-24T16:11:45+0000","dateStarted":"2020-11-24T17:23:02+0000","dateFinished":"2020-11-24T17:23:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2082"},{"text":"%spark.pyspark\n","user":"gaiauser","dateUpdated":"2020-11-24T17:13:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606238004088_279215841","id":"20201124-171324_1960205489","dateCreated":"2020-11-24T17:13:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2083"}],"name":"Good astrometric solutions via ML Random Forrest classifier","id":"2FRPC4BFS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}