{"paragraphs":[{"text":"%spark.pyspark\n# (Q: why am I having to prefix interpreter bindings with \"spark.\" all of a sudden ?!)\n\n# define the data frame source on the given column selection only (we don't want to read the whole thing ... presumably?)\ndf = sqlContext.read.parquet(\"/data/gaia/*.parquet\").select(\"source_id\")\n\n# register as an sql context queryable object\nsqlContext.registerDataFrameAsTable(df, \"gaia_source_ids\")\n","user":"admin","dateUpdated":"2020-12-07T16:42:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579861571971_1838269207","id":"20200124-102611_629820682","dateCreated":"2020-01-24T10:26:11+0000","dateStarted":"2020-12-07T16:42:45+0000","dateFinished":"2020-12-07T16:42:55+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10283"},{"text":"%spark.pyspark\nsql_df = spark.sql(\"SELECT floor(source_id / 562949953421312) AS hpx5, COUNT(*) AS n FROM gaia_source_ids GROUP BY hpx5\")\nprint (sql_df.take(10))\n\n# see paragraph at the end for plotting the results.\n","user":"admin","dateUpdated":"2020-12-07T16:42:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"hpx5":"string","n":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579861901392_188210526","id":"20200124-103141_1498368757","dateCreated":"2020-01-24T10:31:41+0000","dateStarted":"2020-12-07T16:42:58+0000","dateFinished":"2020-12-07T16:44:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10284"},{"text":"%spark.pyspark\n\n# create an RDD from the data frame  (note each element is an RDD row tuple, not a scalar value)\nrdd = df.rdd\nprint ('RDD created from Data Frame has %d partitions by default' % (rdd.getNumPartitions()))\n\n# map at HEALPix level 5\nhealpix_level_5 = rdd.map(lambda x: (x[0] >> 49, 1)) \n#print (healpix_level_5.take(10))\n\n# EUM: don't do this if you can avoid it: use the raw data frame if the API supports the necessary operations since RDD via python are highly sub-optimal.","user":"admin","dateUpdated":"2020-12-07T16:43:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579862300221_1699527242","id":"20200124-103820_550149070","dateCreated":"2020-01-24T10:38:20+0000","dateStarted":"2020-12-07T16:43:02+0000","dateFinished":"2020-12-07T16:44:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10285"},{"text":"%spark.pyspark\n\n# counts by map keys\ncounts_map = healpix_level_5.reduceByKey(lambda x,y: x + y)\n\n# sanity check\nprint(\"Number of unique keys: \",counts_map.count())\nprint(\"Number of partitions: \", counts_map.getNumPartitions())\nprint(counts_map.sample(False, 0.001).collect())\n","user":"gaiauser","dateUpdated":"2020-10-13T14:58:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580121327840_1171810719","id":"20200127-103527_1373318325","dateCreated":"2020-01-27T10:35:27+0000","dateStarted":"2020-10-13T14:58:56+0000","dateFinished":"2020-10-13T15:04:48+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10286"},{"text":"%spark.pyspark\n\n\n# plot up the sky counts\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport healpy as hp\n\n# healpy constants\nNSIDE = 32\nNPIX = hp.nside2npix(NSIDE)\n\n# do the visualisation\narray_like = np.empty(NPIX)\nfor item in counts_map.collect(): array_like[item[0]] = item[1] # by doing it the hard way in plain Spark map/reduce\n#for item in sql_df.rdd.collect():  array_like[item[0]] = item[1] # by doing it the easy SQL aggregate way through Spark SQL context\nhp.mollview(array_like, nest=True, title='Source counts at HEALPix level 5', norm='log')\nhp.graticule(coord='E', color='white')\n","user":"gaiauser","dateUpdated":"2020-10-13T15:04:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580121662753_-1425840286","id":"20200127-104102_1483057249","dateCreated":"2020-01-27T10:41:02+0000","dateStarted":"2020-10-13T15:04:48+0000","dateFinished":"2020-10-13T15:05:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10287"},{"text":"%spark.pyspark\n\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nplt.switch_backend('agg')\nimport numpy as np\nimport healpy as hp\nimport io\n\ndef show(p):\n    img = io.StringIO()\n    p.savefig(img, format='svg')\n    img.seek(0)\n    print (\"%html <div style='width:600px'>\" + img.getvalue() + \"</div>\")\n\n\n# healpy constants\nNSIDE = 32\nNPIX = hp.nside2npix(NSIDE)\n\n# do the visualisation\narray_like = np.empty(NPIX)\nfor item in counts_map.collect(): array_like[item[0]] = item[1] # by doing it the hard way in plain Spark map/reduce\n#for item in sql_df.rdd.collect():  array_like[item[0]] = item[1] # by doing it the easy SQL aggregate way through Spark SQL context\nhp.mollview(array_like, nest=True, title='Source counts at HEALPix level 5', norm='log')\nhp.graticule(coord='E', color='white')\n\n\n\nshow(plt)","user":"gaiauser","dateUpdated":"2020-10-13T15:05:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580123528006_582051126","id":"20200127-111208_1110168867","dateCreated":"2020-01-27T11:12:08+0000","dateStarted":"2020-10-13T15:05:01+0000","dateFinished":"2020-10-13T15:05:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10288"}],"name":"Sky counts map","id":"2F2C6W3GE","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"angular:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}