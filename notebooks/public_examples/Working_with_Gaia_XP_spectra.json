{
  "paragraphs": [
    {
      "title": "Introduction",
      "text": "%md\n\n<!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n    -->\n    \nThe bulk of Gaia XP spectra at Gaia DR3 are provided in a parametric \"continuous\" representational form (as opposed to conventional sampled form, i.e. fluxes in wavelength bins) in table `gaiadr3.xp_continuous_mean_spectrum`. Utilities for handling this form, including conversion to sampled form and plotting, are provided in a bespoke Python package [GaiaXPy](https://gaia-dpci.github.io/GaiaXPy-website/) which is available on this platform. A small subset of the XP spectra are provided also in sampled form in table `gaiadr3.xp_sampled_mean_spectrum` but these are for illustrative purposes only: users are strongly encouraged to familiarise themselves and work with the continuous representation, not least in order to handle correctly the statistical uncertainties inherent to the data.\n\nTo access GaiaXPy on this platform simply import the package as follows:\n\n    import gaiaxpy\n\nthen all classes and utility functions etc. will be available.\n",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T15:20:38+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553427_1722843556",
      "id": "paragraph_1650981001262_1093264483",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:08+0000",
      "dateFinished": "2024-02-29T13:53:08+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:25230"
    },
    {
      "title": "Sampling and plotting spectra (continuous representation)",
      "text": "%pyspark\n\n# standard platform set-up\nimport gaiadmpsetup\n\n# utility code set-up\nfrom gaiaxpy import plot_spectra, convert\n\n# XP products available in Gaia DR3, so set the default database context accordingly for convenience\nspark.sql('USE gaiadr3')\n\n# grab an example spectrum from the table\ncontinuous_df = spark.sql('SELECT * FROM gaiadr3.xp_continuous_mean_spectrum WHERE source_id = 5853498713190525696')\n# ... this source identifier corresponds to Proxima Cen (= Alpha Cen C, spectral type M5.5V i.e. a mid-M dwarf)\n\n# convert to a Pandas dataframe for GaiaXPy\ncontinuous_spectrum = continuous_df.toPandas()\n\n# convert to sampled form:\nsampled_spectrum, sampling = convert(continuous_spectrum, save_file = False)\n    \n# plot to sanity check:\nplot_spectra(sampled_spectrum, sampling = sampling, multi=False, show_plot=True, output_path=None, legend=True)\n\n",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:08+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4041/jobs/job?id=134",
              "$$hashKey": "object:25581"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553427_445437086",
      "id": "paragraph_1650981269844_2057939329",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:08+0000",
      "dateFinished": "2024-02-29T13:53:10+0000",
      "status": "FINISHED",
      "$$hashKey": "object:25231"
    },
    {
      "title": "Creating a single, externally calibrated spectrum from BP and RP",
      "text": "%pyspark\n\nfrom gaiaxpy import calibrate\n\n# GaiaXPy provides classes and methods to create an externally calibrated single spectrum from the internal XP continuous representation:\ncalibrated_spectrum, sampling = calibrate(continuous_spectrum, save_file = False)\n\n# plot it\nplot_spectra(calibrated_spectrum, sampling = sampling, legend = False)\n",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553428_261889787",
      "id": "paragraph_1662044943200_953477984",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:11+0000",
      "dateFinished": "2024-02-29T13:53:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:25232"
    },
    {
      "title": "Searching for similar spectra",
      "text": "%md\n\nThe code in the following cells illustrates a workflow where we trawl through the a large XP spectral data looking for spectra similar to a high signal-to-noise template example. The implementation takes advantage of the end-user programmability of the distributed query execution engine to return results in a reasonable time. The approach is to look for spectra having the same spectral shape, as expressed in the coefficients of the continuous representation.",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:11+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553429_618979145",
      "id": "paragraph_1662045004984_1851487345",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:11+0000",
      "dateFinished": "2024-02-29T13:53:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:25233"
    },
    {
      "title": "Utility code",
      "text": "%pyspark\n\nimport numpy as np\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql import DataFrame\n#from scipy.stats import chi\n\n# static constants for use when reconstructing a covariance matrix from the flattened upper-triangular correlation matrix stored in xp_continuous_mean_spectrum\nNUM_XP_COEFFS = 55\n# these indexes define the column-major positions of the correlation vector elements in the lower triangle of the 2d correlation matrix ...\nlower_index = np.tril_indices(NUM_XP_COEFFS,-1)\n# ... note that numpy indexing is row-major. The upper triangular index is created from the lower, reflecting across the diagonal,\n# by transposing the axes - this results in the required column-major indexing for the upper part\nupper_index = (lower_index[1], lower_index[0])\n# correlation matrix, empty apart from unity on the diagonal (off-diagonal elements to be filled in on a case-by-case basis)\ncorrelation_matrix = np.diag(np.ones(NUM_XP_COEFFS))\n\ndef make_correlation_matrix(correlation_vector : np.ndarray) -> np.ndarray:\n    '''\n    Returns the fully populated 2d correlation matrix given the flattened, 1d upper-triangular off-diagonal\n    elements of the same as persisted in the table of XP continuous representation spectra.\n    '''\n    # copy in unique, off-diagonal elements from the flattened correlation vector into the 2d-indexed positions\n    correlation_matrix[upper_index] = correlation_vector\n    correlation_matrix[lower_index] = correlation_vector\n    \n    # give back the complete correlation matrix\n    return correlation_matrix\n\ndef make_covariance_matrix(complete_correlation_matrix : np.ndarray, coefficient_error_vector : np.ndarray) -> np.ndarray:\n    '''\n    Creates the fully reconstructed 2d covariance matrix of an XP continuous representation spectrum given the\n    complete 2d correlation matrix and the vector of formal errors on the coefficients. Note that Gaia DPAC CU5 scale \n    predicted coefficient errors by the standard deviation as a post-hoc correction to the formal (sqrt) variances.\n    GaiaXPy actually reverses this scaling in computation of the covariance matrix to return the latter as exactly\n    that produced as a result of the least-squares solution for the coefficients. Such a de-scaling of the errors\n    is not applied here: we assume that the uncertainties are best represented with this scaling intact.\n    '''\n    # 2d matrix with the errors on the diagonal\n    error_matrix = np.diag(coefficient_error_vector)\n    \n    # from the standard relationship between covariance and correlation\n    return error_matrix @ (error_matrix @ complete_correlation_matrix)\n    \ndef xp_mahalanobis_distance(coeff_vector_1 : np.ndarray, covariance_1 : np.ndarray,\n        coeff_vector_2 : np.ndarray, error_vector_2 : np.ndarray, correlation_vector_2 : np.ndarray) -> float:\n    '''\n    Computes the Mahalanobis distance between two XP spectra given template coefficients and fully populated\n    covariance for the first, and the data record of the second candidate spectrum, i.e. coefficients, errors and \n    upper-triangular part of the correlation matrix in the continuous representation. The second set of\n    coefficients and errors should be scaled to the same flux level as the template spectrum. This means that\n    the distance returned quantifies how close the candidate SED shape is to that of the template regardless any\n    difference in intrinsic luminosity.\n    \n    This function uses plain matrix inversion of the combined covariance matrix. This can be numerically unstable\n    and result in a negative squared distance resulting in turn in a return value of not-a-number. It is up to \n    the calling application to handle this condition.\n    '''\n    # second covariance matrix\n    corr2 = make_correlation_matrix(correlation_vector_2)\n    covariance_2 = make_covariance_matrix(corr2, error_vector_2)\n    \n    # form covariance of the coefficient difference vector as sum of individual covariance \n    cocovar = covariance_1 + covariance_2\n    \n    # inverse of combined, scaled covariance\n    cocovar_inv = np.linalg.inv(cocovar)\n\n    # use these in the computation of the square of the Mahalanobis distance, e.g. see source linked from\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.mahalanobis.html\n    delta = coeff_vector_1 - coeff_vector_2\n    d = np.sqrt(np.dot(np.dot(delta, cocovar_inv), delta))\n    \n    # resulting Mahalanobs distance follows chi distribution with NUM_XP_COEFFS degrees of freedom (M. Weiler, personal communication)\n    # for identical spectra\n    return d\n    \ndef find_similar_continuous_spectra(data_frame : DataFrame, template_df : DataFrame) -> DataFrame:\n    '''\n    Given data frames defining a large set of XP spectra in continuous representation, \n    and a single template example also in continuous representation, search the former for cases\n    similar to the latter. The data frame of the set of spectra being\n    searched is annotated with a dissimilarity statistic: the greater the value the more\n    dissimilar is the candidate spectrum to the template given. By definition this statistic\n    will be zero for the template spectrum if it is present in the set of candidates.\n    \n    Parameters:\n    -----------\n    data_frame : DataFrame()\n        the data frame encapsulating the set of XP continuous representation spectra to be searched\n    template_df : DataFrame()\n        the template, also in XP continuous representation encapsulated in a data frame.\n        \n    return : DataFrame()\n        a new data frame annotated with a dissimilarity (increasingly positive) statistic where \n        zero indicates a perfect match.\n    '''\n    \n    # convenience reference to template as a Row object:\n    template_row = template_df.collect()[0]\n    \n    # extract the template arrays \n    template_bp_coefficients = np.array(template_row['bp_coefficients']).reshape(-1)\n    template_bp_coefficient_errors = np.array(template_row['bp_coefficient_errors']).reshape(-1)\n    template_bp_correlations = np.array(template_row['bp_coefficient_correlations']).reshape(-1)\n    template_rp_coefficients = np.array(template_row['rp_coefficients']).reshape(-1)\n    template_rp_coefficient_errors = np.array(template_row['rp_coefficient_errors']).reshape(-1)\n    template_rp_correlations = np.array(template_row['rp_coefficient_correlations']).reshape(-1)\n    template_gmag = template_row['phot_g_mean_mag']\n\n    # precompute the required vectors and matrices for the template\n    bp_correl_mat = make_correlation_matrix(template_bp_correlations)\n    template_bp_covariance = make_covariance_matrix(bp_correl_mat, template_bp_coefficient_errors)\n    rp_correl_mat = make_correlation_matrix(template_rp_correlations)\n    template_rp_covariance = make_covariance_matrix(rp_correl_mat, template_rp_coefficient_errors)\n    \n    # define a vectorised Pandas UDF against the template for comparison of other spectra against it\n    @pandas_udf('float')\n    def xp_is_similar(bp_coeffs:pd.Series, bp_coeff_errors:pd.Series, bp_correlations:pd.Series, \n                      rp_coeffs:pd.Series, rp_coeff_errors:pd.Series, rp_correlations:pd.Series,\n                      gmags:pd.Series) -> pd.Series:\n        '''\n        Create a similarity metric for the XP continuous representation coefficients with respect \n        to those of a predefined template. Similarity is based on a combined BP and RP Mahalanobis\n        distance between the coefficient sets with full accounting for covariance. Input arguments\n        are series of the BP and RP coefficients, errors and correlation vectors as stored in table\n        xp_continuous_mean_spectrum. The returned object is a corresponding series of floats of the\n        quadrature sum of the BP and RP Mahalanobis distances between each candidate spectrum and\n        the static template defined above.\n        '''\n        \n        # initialise the results series\n        results = pd.Series(np.full(bp_coeffs.size, 0.0))\n\n        # iterate over the data series\n        for i in range(bp_coeffs.size):\n            \n            # normalise the candidate flux coefficients and uncertainties to that of the template\n            norm = 10.0**(0.4 * (template_gmag - gmags.iloc[i]))\n            bp_coeffs_norm = bp_coeffs.iloc[i] / norm\n            bp_coeff_errors_norm = bp_coeff_errors.iloc[i] / norm\n            rp_coeffs_norm = rp_coeffs.iloc[i] / norm\n            rp_coeff_errors_norm = rp_coeff_errors.iloc[i] / norm\n            \n            # Mahalanobis distances for the individual BP and RP coefficient sets normalised to the flux level of the template\n            bp_mdist = xp_mahalanobis_distance(template_bp_coefficients, template_bp_covariance, bp_coeffs_norm, bp_coeff_errors_norm, bp_correlations.iloc[i])\n            rp_mdist = xp_mahalanobis_distance(template_rp_coefficients, template_rp_covariance, rp_coeffs_norm, rp_coeff_errors_norm, rp_correlations.iloc[i])\n            \n            # combined Mahalanobis distance\n            results.iloc[i] = np.sqrt(bp_mdist * bp_mdist + rp_mdist * rp_mdist)\n            \n        return results\n\n    # add in the similarity statistic\n    data_frame = data_frame.withColumn('xp_similar', xp_is_similar(\n        data_frame.bp_coefficients, data_frame.bp_coefficient_errors, data_frame.bp_coefficient_correlations, \n        data_frame.rp_coefficients, data_frame.rp_coefficient_errors, data_frame.rp_coefficient_correlations, data_frame.phot_g_mean_mag))\n        \n    # give back the full set filtering out any nulls (which may result from NaN individual Mahalanobis distances)\n    return data_frame.filter(data_frame.xp_similar.isNotNull())\n\n",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553429_535120608",
      "id": "paragraph_1662045098333_1096300336",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:11+0000",
      "dateFinished": "2024-02-29T13:53:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:25234"
    },
    {
      "title": "Set up the template and selection for the trawl",
      "text": "%pyspark\n\n# select a high s/n template for the search, e.g. Proxima Cen\nsid = 5853498713190525696\n\n# defined the template data frame\ntemplate_df = spark.sql('SELECT xp.*, g.phot_g_mean_mag FROM xp_continuous_mean_spectrum AS xp INNER JOIN gaia_source AS g ON g.source_id = xp.source_id WHERE g.source_id = %d'%(sid))\n\n# define a query over the entire dataset, restricting to low reddening for simplicity\nquery = 'SELECT xp.*, g.phot_g_mean_mag  ' + \\\n        'FROM xp_continuous_mean_spectrum AS xp INNER JOIN gaia_source AS g ON g.source_id = xp.source_id ' + \\\n        'WHERE g.ag_gspphot < 0.1 AND MOD(g.random_index, 20) = 0'\n# TEST: give the template only in the df\n#query = 'SELECT * FROM xp_continuous_mean_spectrum WHERE source_id = %d'%(sid)\n\n# sanity check the formatted query\n#print(query)\n\n# define a data frame via the query\ndf = spark.sql(query)\n\n# get any that are similar and show them\nsimilar_df = find_similar_continuous_spectra(df, template_df)\n\n# results quick-look\n#similar_df.show()\n",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4041/jobs/job?id=135",
              "$$hashKey": "object:25753"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553429_1147158826",
      "id": "paragraph_1662045989203_497219070",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:11+0000",
      "dateFinished": "2024-02-29T13:53:14+0000",
      "status": "FINISHED",
      "$$hashKey": "object:25235"
    },
    {
      "title": "Action the trawl and collect the top 3 (for example) matches",
      "text": "%pyspark\n\n# collecting the results to a Pandas data frame collects the results to the driver interpreter process\n# so this cell will actually action the trawl on the Spark worker cluster. Be prepared to sit back and wait...\ntop3_pdf = similar_df.sort(similar_df.xp_similar.asc()).limit(3).toPandas()\n\n# sanity check as required\n# z.show(top3_pdf)",
      "user": "gaiauser",
      "dateUpdated": "2024-02-29T13:53:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4041/jobs/job?id=136",
              "$$hashKey": "object:25801"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553429_725211401",
      "id": "paragraph_1662046085285_632981835",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "dateStarted": "2024-02-29T13:53:14+0000",
      "dateFinished": "2024-02-29T13:55:08+0000",
      "status": "ERROR",
      "$$hashKey": "object:25236"
    },
    {
      "title": "Plot sampled spectra (internal calibration)",
      "text": "%pyspark\n\n# convert to sampled form:\nsampled_spectra, sampling = convert(top3_pdf, save_file = False)\n    \n# plot to sanity check:\nplot_spectra(sampled_spectra, sampling = sampling, multi=True, show_plot=True, output_path=None, legend=True)\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-01-25T14:05:53+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553429_1779642416",
      "id": "paragraph_1662046203039_188861744",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "status": "READY",
      "$$hashKey": "object:25237"
    },
    {
      "title": "Plot externally calibrated spectra",
      "text": "%pyspark\n\n# externally calibrate the spectra\ncalibrated_spectra, sampling = calibrate(top3_pdf, save_file = False)\n\n# plot the spectra\nplot_spectra(calibrated_spectra, sampling = sampling, legend = True)\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-01-25T14:05:53+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553430_1544666297",
      "id": "paragraph_1662046259646_282464556",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "status": "READY",
      "$$hashKey": "object:25238"
    },
    {
      "title": "Further information",
      "text": "%md\n\n* [Gaia DR3 XP spectra online documentation](https://gea.esac.esa.int/archive/documentation/GDR3/Data_processing/chap_cu5pho/cu5pho_sec_specProcessing/cu5pho_ssec_specInternCal.html#SSS3)\n* [Gaia DR3 spectroscopic data model](https://gea.esac.esa.int/archive/documentation/GDR3/Gaia_archive/chap_datamodel/sec_dm_spectroscopic_tables/)\n* [GaiaXPy website](https://gaia-dpci.github.io/GaiaXPy-website/)\n* [GaiaXPy API documentation](https://gaiaxpy.readthedocs.io/en/latest/gaiaxpy.html)\n* [Vectorised User Defined Functions for PySpark](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)\n",
      "user": "anonymous",
      "dateUpdated": "2023-01-25T14:05:53+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1674655553432_1599662687",
      "id": "paragraph_1651053931648_1906164321",
      "dateCreated": "2023-01-25T14:05:53+0000",
      "status": "READY",
      "$$hashKey": "object:25239"
    }
  ],
  "name": "5. Working with Gaia XP spectra",
  "id": "2HQUJSZ9C",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {
    "isRunning": false
  },
  "path": "/tmp/5. Working with Gaia XP spectra"
}
